{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyON+wUQnVadzazG2kr6HlKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leotodisco/SmolLM-Fine-Tuning/blob/main/SmolLM_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q6JtEG2inq-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 --progress-bar off\n",
        "!pip install -qqq flash-attn --no-build-isolation --progress-bar off\n"
      ],
      "metadata": {
        "id": "hEWKdEyHX9Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from transformers import BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "PAmrrUjZYFp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load The Dataset\n",
        "\n",
        "## Task: reddit posts summarization"
      ],
      "metadata": {
        "id": "knt5ajdJYNqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"mlabonne/smoltldr\")"
      ],
      "metadata": {
        "id": "eHcmv9hMYMka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"prompt\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "D-kAFwmWZSEO",
        "outputId": "5f5931d3-faeb-40f8-eba3-fc37ea9a6503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"SUBREDDIT: r/tifu\\n\\nTITLE: TIFU by trying to pet a dog.\\n\\nPOST: Last night I went to a Hippie May Day Festival/ Camp out. Needless to say, I passed out hard in my tent at the end of the night.Woken by the warmth and light of the morning sun, I emerged from my tent in search of some water to quench my burgeoning thirst. To my delight I spotted a dog scouting the field before me, about 110 meters away. Without delay I dashed towards it, my urge to pet this dog was immeasurable. On the way back to my tent, while running, I just so happened to come upon the most heinous stick I have ever encountered. The bastard was sticking straight out of the earth, cleverly hidden in a plush, verdant meadow. My foot never saw it coming. It had no warning, no shoe, no defense! The poor soul (no pun intended) never saw it coming, until the moment it was impaled by what I would have thought was the devils pitchfork itself. The worst part is, I didn't even get to pet the dog, it sprinted back to it's owners property when it saw me approaching.\\n\\nTL;DR:\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"completion\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5SF2Q-nFZaxN",
        "outputId": "cb8fa25f-2a24-49a7-e749-5e349f748b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Tried to pet a dog, foot got impaled by a demon stick, never even got to pet the dog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is composed of:\n",
        "- Prompts\n",
        "- Completions\n",
        "\n",
        "We have 2000 prompts and 2000 completions\n"
      ],
      "metadata": {
        "id": "aCRSl8BwaF98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Rlilu8bIaHyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"!sudo add-apt-repository ppa:ubuntu-toolchain-r/test\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install gcc-4.9\n",
        "!sudo apt-get upgrade libstdc++6\"\"\""
      ],
      "metadata": {
        "id": "BQLum7AndfzY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "07b96b16-4228-4bc5-db93-b141b8291a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!sudo add-apt-repository ppa:ubuntu-toolchain-r/test\\n!sudo apt-get update\\n!sudo apt-get install gcc-4.9\\n!sudo apt-get upgrade libstdc++6'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=\"all-linear\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "buOJPvKub769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69e692d-be47-429a-fadb-09f14daba85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,442,240 || all params: 136,957,248 || trainable%: 1.7832\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Function\n"
      ],
      "metadata": {
        "id": "lnEpLeC4fKtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward function\n",
        "ideal_length = 50\n",
        "\n",
        "\n",
        "def reward_len(completions, **kwargs):\n",
        "    return [-abs(ideal_length - len(completion)) for completion in completions]"
      ],
      "metadata": {
        "id": "qQduk1HvfJva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = GRPOConfig(\n",
        "    output_dir=\"GRPO\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    max_prompt_length=512,\n",
        "    max_completion_length=96,\n",
        "    num_generations=8,\n",
        "    optim=\"adamw_8bit\",\n",
        "    num_train_epochs=1,\n",
        "    bf16=True,\n",
        "    remove_unused_columns=False,\n",
        "    logging_steps=1,\n",
        "    report_to=[\"none\"]\n",
        ")"
      ],
      "metadata": {
        "id": "4SSJXmeOfQQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=[reward_len],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "v8qnS6Dsfcxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda5f80e-f4f5-4373-c393-6a893c22bc03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}